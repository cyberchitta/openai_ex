# OpenaiEx User Guide

```elixir
Mix.install([
  # {:openai_ex, "~> 0.5.9"},
  {:openai_ex, path: Path.join(__DIR__, "..")},
  {:kino, "~> 0.12.3"}
])
```

## Introduction

`OpenaiEx` is an Elixir library that provides a community-maintained OpenAI API client especially for Livebook development.

Portions of this project were developed with assistance from ChatGPT 3.5 and 4.

At this point, all API endpoints and features (as of Nov 15, 2023) are supported, including the **Assistants API Beta**, DALL-E-3, Text-to-Speech, the **tools support** in chat completions, and the **streaming version** of the chat completion endpoint. Streaming request **cancellation** is also supported.

Configuration of Finch pools and API base url are supported.

There are some differences compared to other elixir openai wrappers.

* I tried to faithfully mirror the naming/structure of the official python api. For example, content that is already in memory can be uploaded as part of a request, it doesn't have to be read from a file at a local path.
* I was developing for a livebook use-case, so I don't have any config, only environment variables.
* Streaming API versions, with request cancellation, are supported.
* The underlying transport is finch, rather than httpoison
* 3rd Party (including local) LLMs with an OpenAI proxy, as well as the **Azure OpenAI API**, are considered legitimate use cases.

To learn how to use OpenaiEx, you can refer to the relevant parts of the official OpenAI API reference documentation, which we link to throughout this document.

This file is an executable Livebook, which means you can interactively run and modify the code samples provided. We encourage you to open it in Livebook and try out the code for yourself!

## Installation

You can install OpenaiEx using Mix:

### In Livebook

Add the following code to the first connection cell:

<!-- livebook:{"force_markdown":true} -->

```elixir
Mix.install(
  [
    {:openai_ex, "~> 0.5.9"}
  ]
)
```

### In a Mix Project

Add the following to your mix.exs file:

<!-- livebook:{"force_markdown":true} -->

```elixir
def deps do
  [
    {:openai_ex, "~> 0.5.9"}
  ]
end
```

## Authentication

To authenticate with the OpenAI API, you will need an API key. We recommend storing your API key in an environment variable. Since we are using Livebook, we can store this and other environment variables as [Livebook Hub Secrets](https://news.livebook.dev/hubs-and-secret-management---launch-week-1---day-3-3tMaJ2).

```elixir
apikey = System.fetch_env!("LB_OPENAI_API_KEY")
openai = OpenaiEx.new(apikey)
```

You can also specify an organization if you are a member of more than one:

```elixir
# organization = System.fetch_env!("LB_OPENAI_ORGANIZATION")
# openai = OpenaiEx.new(apikey, organization)
```

For more information on authentication, see the [OpenAI API Authentication reference](https://platform.openai.com/docs/api-reference/authentication).

### Configuration

There are a few places where configuration seemed necessary.

#### Receive Timeout

The default receive timeout is 15 seconds. If you are seeing longer latencies, you can override the default with

```elixir
# set receive timeout to 45 seconds
openai = OpenaiEx.new(apikey) |> OpenaiEx.with_receive_timeout(45_000)
```

#### Finch Instance

In production scenarios where you want to explicitly tweak the Finch pool, you can create a new Finch instance using

<!-- livebook:{"force_markdown":true} -->

```elixir
Finch.start_link(
    name: MyConfguredFinch,
    pools: ...
)
```

You can use this instance of Finch (instead of the default `OpenaiEx.Finch`) by setting the finch name

<!-- livebook:{"force_markdown":true} -->

```elixir
openai_with_custom_finch = openai |> with_finch_name(MyConfiguredFinch)
```

<!-- livebook:{"break_markdown":true} -->

#### Base Url

There are times, such as when using a local LLM (like Ollama) with an OpenAI proxy, when you need to reset the base url of the API. This is generally only applicable for chat and chat completion endpoints and can be accomplished by

```elixir
# in this example, our development livebook server is running in a docker dev container while 
# the local llm is running on the host machine
proxy_openai =
  OpenaiEx.new(apikey) |> OpenaiEx.with_base_url("http://host.docker.internal:8000/v1")
```

#### Azure OpenAI

The Azure OpenAI API replicates the Completion, Chat Completion and Embeddings endpoints from OpenAI.

However, it modifies the base URL as well as the endpoint path, and adds a parameter to the URL query. These modifications are accommodated with the following calls:

for non Entra Id

<!-- livebook:{"force_markdown":true} -->

```elixir
openai = OpenaiEx._for_azure(azure_api_id, resource_name, deployment_id, api_version)
```

and for Entra Id

<!-- livebook:{"force_markdown":true} -->

```elixir
openai = OpenaiEx.new(entraId) |> OpenaiEx._for_azure(resource_name, deployment_id, api_version)
```

These methods will be supported as long as the Azure version does not deviate too far from the base OpenAI API.

## Model

### List Models

To list all available models, use the [`Model.list()`](https://platform.openai.com/docs/api-reference/models/list) function:

```elixir
alias OpenaiEx.Models

openai |> Models.list()
```

### Retrieve Models

To retrieve information about a specific model, use the [`Model.retrieve()`](https://platform.openai.com/docs/api-reference/models/retrieve) function:

```elixir
openai |> Models.retrieve("gpt-3.5-turbo")
```

For more information on using models, see the [OpenAI API Models reference](https://platform.openai.com/docs/api-reference/models).

## Chat Completion

To generate a chat completion, you need to define a chat completion request structure using the `ChatCompletion.new()` function. This function takes several parameters, such as the model ID and a list of chat messages. We have a module `ChatMessage` which helps create messages in the [chat format](https://platform.openai.com/docs/guides/chat/introduction).

```elixir
alias OpenaiEx.Chat.Completions
alias OpenaiEx.ChatMessage
alias OpenaiEx.MsgContent

chat_req =
  Completions.new(
    model: "gpt-3.5-turbo",
    messages: [
      ChatMessage.user(
        "Give me some background on the elixir language. Why was it created? What is it used for? What distinguishes it from other languages? How popular is it?"
      )
    ]
  )
```

You are able to pass images to the API by creating a message.

```elixir
ChatMessage.user(
  MsgContent.image_url(
    "https://raw.githubusercontent.com/restlessronin/openai_ex/main/assets/images/starmask.png"
  )
)
```

You can generate a chat completion using the [`ChatCompletion.create()`](https://platform.openai.com/docs/api-reference/chat/completions/create) function:

```elixir
chat_response = openai |> Completions.create(chat_req)
```

For a more in-depth example of `ChatCompletion`, check out the [Deeplearning.AI OrderBot Livebook](https://hexdocs.pm/openai_ex/dlai_orderbot.html).

You can also call the endpoint and have it stream the response. This returns the result as a series of tokens, which have to be put together in code.

To use the stream option, call the `ChatCompletion.create()` function with `stream: true`

```elixir
chat_stream = openai |> Completions.create(chat_req, stream: true)
IO.puts(inspect(chat_stream))
IO.puts(inspect(chat_stream.task_pid))
chat_stream.body_stream |> Stream.flat_map(& &1) |> Enum.each(fn x -> IO.puts(inspect(x)) end)
```

The `chat_stream.task_pid` can be used in conjunction with `OpenaiEx.HttpSse.cancel_request/1` to cancel an ongoing request.

For a detailed example of the use of the streaming `ChatCompletion` API, **including how to cancel an ongoing request**, check out [Streaming Orderbot](https://hexdocs.pm/openai_ex/streaming_orderbot.html), the streaming equivalent of the prior example.

For more information on generating chat completions, see the [OpenAI API Chat Completions reference](https://platform.openai.com/docs/api-reference/chat).

### Function(Tool) Calling

In OpenAI's `ChatCompletion` endpoint, you can use the function calling feature to call a custom function and pass its result as part of the conversation. Here's an example of how to use the function calling feature:

First, we set up the function specification and completion request. The function specification defines the name, description, and parameters of the function we want to call. In this example, we define a function called `get_current_weather` that takes a `location` parameter and an optional `unit` parameter. The completion request includes the function specification, the conversation history, and the model we want to use.

```elixir
tool_spec =
  Jason.decode!("""
    {"type": "function",
     "function": {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  """)

rev_msgs = [
  ChatMessage.user("What's the weather like in Boston today?")
]

fn_req =
  Completions.new(
    model: "gpt-3.5-turbo",
    messages: rev_msgs |> Enum.reverse(),
    tools: [tool_spec],
    tool_choice: "auto"
  )
```

Next, we call the OpenAI endpoint to get a response that includes the function call.

```elixir
fn_response = openai |> Completions.create(fn_req)
```

We extract the function call from the response and call the appropriate function with the given parameters. In this example, we define a map of functions that maps function names to their implementations. We then use the function name and arguments from the function call to look up the appropriate function and call it with the given parameters.

```elixir
fn_message = fn_response["choices"] |> Enum.at(0) |> Map.get("message")
tool_call = fn_message |> Map.get("tool_calls") |> List.first()
tool_id = tool_call |> Map.get("id")
fn_call = tool_call |> Map.get("function")

functions = %{
  "get_current_weather" => fn location, unit ->
    %{
      "location" => location,
      "temperature" => "72",
      "unit" => unit,
      "forecast" => ["sunny", "windy"]
    }
    |> Jason.encode!()
  end
}

fn_name = fn_call["name"]
fn_args = fn_call["arguments"] |> Jason.decode!()

location = fn_args["location"]
unit = unless is_nil(fn_args["unit"]), do: fn_args["unit"], else: "fahrenheit"

fn_value = functions[fn_name].(location, unit)
```

We then pass the returned value back to the ChatCompletion endpoint with the conversation history to that point to get the final response.

```elixir
latest_msgs = [ChatMessage.tool(tool_id, fn_name, fn_value) | [fn_message | rev_msgs]]

fn_req_2 =
  Completions.new(
    model: "gpt-3.5-turbo",
    messages: latest_msgs |> Enum.reverse()
  )

fn_response_2 = openai |> Completions.create(fn_req_2)
```

The final response includes the result of the function call integrated into the conversation.

## Image

### Generate Image

We define the image creation request structure using the `Image.new` function

```elixir
alias OpenaiEx.Images

img_req = Images.Generate.new(prompt: "An adorable baby sea otter", size: "256x256", n: 1)
```

Then call the [`Image.create()`](https://platform.openai.com/docs/api-reference/images/create) function to generate the images.

```elixir
img_response = openai |> Images.generate(img_req)
```

For more information on generating images, see the [OpenAI API Image reference](https://platform.openai.com/docs/api-reference/images).

#### Fetch the generated images

With the information in the image response, we can fetch the images from their URLs

```elixir
fetch_blob = fn url ->
  Finch.build(:get, url) |> Finch.request!(OpenaiEx.Finch) |> Map.get(:body)
end
```

```elixir
fetched_images = img_response["data"] |> Enum.map(fn i -> i["url"] |> fetch_blob.() end)
```

#### View the generated images

Finally, we can render the images using Kino

```elixir
fetched_images
|> Enum.map(fn r -> r |> Kino.Image.new("image/png") |> Kino.render() end)
```

```elixir
img_to_expmt = fetched_images |> List.first()
```

### Edit Image

We define an image edit request structure using the `Images.Edit.new()` function. This function requires an image and a mask. For the image, we will use the one that we received. Let's load the mask from a URL.

```elixir
# if you're having problems downloading raw github content, you may need to manually set your DNS server to "8.8.8.8" (google)
star_mask =
  fetch_blob.(
    "https://raw.githubusercontent.com/restlessronin/openai_ex/main/assets/images/starmask.png"
  )

# star_mask = OpenaiEx.new_file(path: Path.join(__DIR__, "../assets/images/starmask.png"))
```

Set up the image edit request with image, mask and prompt.

```elixir
img_edit_req =
  Images.Edit.new(
    image: img_to_expmt,
    mask: star_mask,
    size: "256x256",
    prompt: "Image shows a smiling Otter"
  )
```

We then call the [`Image.create_edit()`]() function

```elixir
img_edit_response = openai |> Images.edit(img_edit_req)
```

and view the result

```elixir
img_edit_response["data"]
|> Enum.map(fn i -> i["url"] |> fetch_blob.() |> Kino.Image.new("image/png") |> Kino.render() end)
```

### Image Variations

We define an image variation request structure using the `Images.Variation.new()` function. This function requires an image.

```elixir
img_var_req = Images.Variation.new(image: img_to_expmt, size: "256x256")
```

Then call the [`Image.create_variation()`](https://platform.openai.com/docs/api-reference/images/create-variation) function to generate the images.

<!-- livebook:{"break_markdown":true} -->

###

```elixir
img_var_response = openai |> Images.create_variation(img_var_req)
```

```elixir
img_var_response["data"]
|> Enum.map(fn i -> i["url"] |> fetch_blob.() |> Kino.Image.new("image/png") |> Kino.render() end)
```

For more information on images variations, see the [OpenAI API Image Variations reference](https://platform.openai.com/docs/api-reference/images/create-variation).

## Embedding

Define the embedding request structure using `Embedding.new`.

```elixir
alias OpenaiEx.Embeddings

emb_req =
  Embeddings.new(
    model: "text-embedding-ada-002",
    input: "The food was delicious and the waiter..."
  )
```

Then call the [`Embedding.create()`]() function.

```elixir
emb_response = openai |> Embeddings.create(emb_req)
```

For more information on generating embeddings, see the [OpenAI API Embedding reference](https://platform.openai.com/docs/api-reference/embeddings/create)

## Audio

```elixir
alias OpenaiEx.Audio
```

### Create speech

<!-- livebook:{"break_markdown":true} -->

For text to speech, we create an `Audio.Speech` request structure as follows

```elixir
speech_req =
  Audio.Speech.new(
    model: "tts-1",
    voice: "alloy",
    input: "The quick brown fox jumped over the lazy dog",
    response_format: "mp3"
  )
```

We then call the [`Audio.Speech.create()`](https://platform.openai.com/docs/api-reference/audio/createSpeech) function to create the audio response

```elixir
speech_response = openai |> Audio.Speech.create(speech_req)
```

We can play the response using the `Kino` Audio widget.

```elixir
speech_response |> Kino.Audio.new(:mp3)
```

### Create transcription

To define an audio transcription request structure, we need to create a file parameter using `Audio.File.new()`.

```elixir
# if you're having problems downloading raw github content, you may need to manually set your DNS server to "8.8.8.8" (google)
audio_url = "https://raw.githubusercontent.com/restlessronin/openai_ex/main/assets/transcribe.mp3"
audio_file = OpenaiEx.new_file(name: audio_url, content: fetch_blob.(audio_url))

# audio_file = OpenaiEx.new_file(path: Path.join(__DIR__, "../assets/transcribe.mp3"))
```

The file parameter is used to create the Audio.Transcription request structure

```elixir
translation_req = Audio.Transcription.new(file: audio_file, model: "whisper-1")
```

We then call the [`Audio.Transcription.create()`](https://platform.openai.com/docs/api-reference/audio/createTranscription) function to create a transcription.

```elixir
translation_response = openai |> Audio.Transcription.create(translation_req)
```

### Create translation

The translation call uses practically the same request structure, but calls the [`Audio.Translation.create()`](https://platform.openai.com/docs/api-reference/audio/createTranslation) endpoint

For more information on the audio endpoints see the [Openai API Audio Reference](https://platform.openai.com/docs/api-reference/audio)

## File

### List files

To request all files that belong to the user organization, call the [`File.list()`](https://platform.openai.com/docs/api-reference/files/list) function

```elixir
alias OpenaiEx.Files

openai |> Files.list()
```

### Upload files

To upload a file, we need to create a file parameter, and then the upload request

```elixir
# if you're having problems downloading raw github content, you may need to manually set your DNS server to "8.8.8.8" (google)
ftf_url = "https://raw.githubusercontent.com/restlessronin/openai_ex/main/assets/fine-tune.jsonl"
fine_tune_file = OpenaiEx.new_file(name: ftf_url, content: fetch_blob.(ftf_url))

# fine_tune_file = OpenaiEx.new_file(path: Path.join(__DIR__, "../assets/fine-tune.jsonl"))

upload_req = Files.new_upload(file: fine_tune_file, purpose: "fine-tune")
```

Then we call the [`File.create()`](https://platform.openai.com/docs/api-reference/files/upload) function to upload the file

```elixir
upload_res = openai |> Files.create(upload_req)
```

We can verify that the file has been uploaded by calling

```elixir
openai |> Files.list()
```

We grab the file id from the previous response value to use in the following samples

```elixir
file_id = upload_res["id"]
```

### Retrieve files

In order to retrieve meta information on a file, we simply call the [`File.retrieve()`]() function with the given id

```elixir
openai |> Files.retrieve(file_id)
```

### Retrieve file content

Similarly to download the file contents, we call [`File.download()`]()

```elixir
openai |> Files.download(file_id)
```

### Delete file

Finally, we can delete the file by calling [`File.delete()`](https://platform.openai.com/docs/api-reference/files/delete)

```elixir
openai |> Files.delete(file_id)
```

Verify that the file has been deleted by listing files again

```elixir
openai |> Files.list()
```

## FineTuning Job

To run a fine-tuning job, we minimally need a training file. We will re-run the file creation request above.

```elixir
upload_res = openai |> Files.create(upload_req)
```

Next we call `FineTuning.Jobs.new()` to create a new request structure

```elixir
alias OpenaiEx.FineTuning

ft_req = FineTuning.Jobs.new(model: "davinci-002", training_file: upload_res["id"])
```

To begin the fine tune, we call the [`FineTuning.Jobs.create()`](https://platform.openai.com/docs/api-reference/fine-tunes/create) function

```elixir
ft_res = openai |> FineTuning.Jobs.create(ft_req)
```

We can list all fine tunes by calling [`FineTuning.Jobs.list()`](https://platform.openai.com/docs/api-reference/fine-tunes/list)

```elixir
openai |> FineTuning.Jobs.list()
```

The function [`FineTune.retrieve()`](https://platform.openai.com/docs/api-reference/fine-tunes/retrieve) gets the details of a particular fine tune.

```elixir
ft_id = ft_res["id"]
openai |> FineTuning.Jobs.retrieve(fine_tuning_job_id: ft_id)
```

and [`FineTuning.Jobs.list_events()`](https://platform.openai.com/docs/api-reference/fine-tunes/events) can be called to get the events

```elixir
openai |> FineTuning.Jobs.list_events(fine_tuning_job_id: ft_id)
```

To cancel a Fine Tune job, call [`FineTuning.Jobs.cancel()`](https://platform.openai.com/docs/api-reference/fine-tunes/cancel)

```elixir
openai |> FineTuning.Jobs.cancel(fine_tuning_job_id: ft_id)
```

A fine tuned model can be deleted by calling the [`Model.delete()`](https://platform.openai.com/docs/api-reference/fine-tunes/delete-model)

```elixir
ft_model = ft_res["fine_tuned_model"]

unless is_nil(ft_model) do
  openai |> Models.delete(ft_model)
end
```

For more information on the fine tune endpoints see the [Openai API Moderation Reference](https://platform.openai.com/docs/api-reference/fine-tunes)

## Moderation

We use the moderation API by calling `Moderation.new()` to create a new request

```elixir
alias OpenaiEx.Moderations

mod_req = Moderations.new(input: "I want to kill people")
```

The call the function [`Moderation.create()`](https://platform.openai.com/docs/api-reference/moderations/create)

```elixir
mod_res = openai |> Moderations.create(mod_req)
```

For more information on the moderation endpoints see the [Openai API Moderation Reference](https://platform.openai.com/docs/api-reference/moderations)

## Assistant

```elixir
alias OpenaiEx.Beta.Assistants
```

### Create Assistant

To create an assistant with model and instructions, call the [`Assistant.create()`](https://platform.openai.com/docs/api-reference/assistants/createAssistant) function.

First, we setup the create request parameters. This request sets up an Assistant with a code interpreter tool.

```elixir
math_assistant_req =
  Assistants.new(
    instructions:
      "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",
    name: "Math Tutor",
    tools: [%{type: "code_interpreter"}],
    model: "gpt-4"
  )
```

Then we call the create function

```elixir
asst = openai |> Assistants.create(math_assistant_req)
```

### Retrieve Assistant

Extract the id field for the assistant

```elixir
assistant_id = asst["id"]
```

which can then be used to retrieve the Assistant fields, using the [`Assistant.retrieve()`](https://platform.openai.com/docs/api-reference/assistants/getAssistant) function.

```elixir
openai |> Assistants.retrieve(assistant_id)
```

### Modify Assistant

Once created, an assistant can be modified using the [`Assistant.update()`](https://platform.openai.com/docs/api-reference/assistants/modifyAssistant) function.

Now we will show an example assistant request using the retrieval tool with a set of files. First we set up the files (in this case a sample HR document) by uploading using the `File` API.

```elixir
alias OpenaiEx.Files

hr_file = OpenaiEx.new_file(path: Path.join(__DIR__, "../assets/cyberdyne.txt"))
hr_upload_req = Files.new_upload(file: hr_file, purpose: "assistants")
hr_upload_res = openai |> Files.create(hr_upload_req)
```

```elixir
file_id = hr_upload_res["id"]
```

Next we create the update request

```elixir
hr_assistant_req =
  Assistants.new(
    instructions:
      "You are an HR bot, and you have access to files to answer employee questions about company policies. Always respond with info from one of the files.",
    name: "HR Helper",
    tools: [%{type: "retrieval"}],
    model: "gpt-3.5-turbo-1106",
    file_ids: [file_id]
  )
```

Finally we call the endpoint to modify the `Assistant`

```elixir
asst = openai |> Assistants.update(assistant_id, hr_assistant_req)
```

### Delete Assistant

Finally we can delete assistants using the [`Assistant.delete()`](https://platform.openai.com/docs/api-reference/assistants/deleteAssistant) function

```elixir
openai |> Assistants.delete(assistant_id)
```

### List Assistants

We use [`Assistant.list()`](https://platform.openai.com/docs/api-reference/assistants/listAssistants) to get a list of assistants

```elixir
assts = openai |> Assistants.list()
```

### Create Assistant File

Use [`Assistants.File.create()`](https://platform.openai.com/docs/api-reference/assistants/createAssistantFile) to attach a file to an assistant.

Let us re-create the first assistant above

```elixir
math_asst = openai |> Assistants.create(math_assistant_req)
```

Now we can attach the file that we used earlier to this new assistant.

```elixir
math_assistant_id = math_asst["id"]
asst_f = Assistants.File.new(assistant_id: math_assistant_id, file_id: file_id)
asst_file = openai |> Assistants.File.create(asst_f)
```

Let's retrieve the assistant to see if it was updated with the file id  (check `file_ids` field value).

```elixir
openai |> Assistants.retrieve(math_assistant_id)
```

### Retreive Assistant File

We can retrieve an assistant file by using [`Assistants.File.retrieve()`](https://platform.openai.com/docs/api-reference/assistants/getAssistantFile)

```elixir
openai |> Assistants.File.retrieve(asst_f)
```

### Delete Assistant File

[`Assistant.File.delete()`] can be used to detach a file from the assistant.

```elixir
openai |> Assistants.File.delete(asst_f)
```

Verify detach by retrieving the assistant again (check `file_ids` field value)

```elixir
openai |> Assistants.retrieve(math_assistant_id)
```

### List Assistant Files

We can list all files attached to an assistant with [`Assistant.File.list()`](https://platform.openai.com/docs/api-reference/assistants/listAssistantFiles)

```elixir
list_req = Assistants.File.new_list(assistant_id: math_assistant_id)

openai |> Assistants.File.list(list_req)
```

## Thread

```elixir
alias OpenaiEx.Beta.Threads
```

### Create thread

Use the [`Thread.create()`] function to create threads. A thread can be created empty or with messages.

```elixir
alias OpenaiEx.ChatMessage

empty_thread = openai |> Threads.create()

msg_hr = ChatMessage.user("What company do we work at?", [file_id])
msg_ai = ChatMessage.user("How does AI work? Explain it in simple terms.")

thrd_req = Threads.new(messages: [msg_hr, msg_ai])

thread = openai |> Threads.create(thrd_req)
```

### Retrieve thread

[`Thread.retrieve()`](https://platform.openai.com/docs/api-reference/threads/getThread) can be used to get the thread parameters given the id.

```elixir
thread_id = thread["id"]
openai |> Threads.retrieve(thread_id)
```

### Modify thread

The metadata for a thread can be modified using [`Thread.update()`](https://platform.openai.com/docs/api-reference/threads/modifyThread)

```elixir
openai |> Threads.update(thread_id, %{metadata: %{modified: "true", user: "abc123"}})
```

### Delete thread

Use [`Thread.delete()`](https://platform.openai.com/docs/api-reference/threads/deleteThread) to delete a thread

```elixir
openai |> Threads.delete(thread_id)
```

Verify deletion

```elixir
openai |> Threads.retrieve(thread_id)
```

## Messages

```elixir
alias OpenaiEx.Beta.Threads.Messages
```

### Create message

You can create a single message for a thread using [`Message.create()`](https://platform.openai.com/docs/api-reference/messages/createMessage)

```elixir
thread_id = empty_thread["id"]

message = openai |> Messages.create(thread_id, msg_hr)
```

### Retrieve message

Use [`Message.retrieve()`] to retrieve a message

```elixir
message_id = message["id"]
openai |> Messages.retrieve(%{thread_id: thread_id, message_id: message_id})
```

### Modify message

The metadata for a message can be modified by [`Message.update()`]

```elixir
metadata = %{modified: "true", user: "abc123"}
upd_msg_req = Messages.new(thread_id: thread_id, message_id: message_id, metadata: metadata)

message = openai |> Messages.update(upd_msg_req)
```

### List messages

Use [`Message.list()`] to get all the messages for a given thread

```elixir
openai |> Messages.list(thread_id)
```

### Retrieve message file

Retrieve a message file using [`Message.File.retrieve()`]

```elixir
openai
|> Messages.File.retrieve(%{thread_id: thread_id, message_id: message_id, file_id: file_id})
```

### List message files

You can list all message files using [`Message.File.list()`]

```elixir
openai |> Messages.File.list(%{thread_id: thread_id, message_id: message_id})
```

## Runs

```elixir
alias OpenaiEx.Beta.Threads.Runs
```

### Create run

A run represents an execution on a thread. Use to [`Run.create()`](https://platform.openai.com/docs/api-reference/runs/createRun) with an assistant on a thread

```elixir
run_req = Runs.new(thread_id: thread_id, assistant_id: math_assistant_id)

run = openai |> Runs.create(run_req)
```

### Retrieve run

Retrieve a run using [`Run.retrieve()`](https://platform.openai.com/docs/api-reference/runs/getRun)

```elixir
run_id = run["id"]
openai |> Runs.retrieve(%{thread_id: thread_id, run_id: run_id})
```

### Modify run

The run metadata can be modified using the [`Run.update()`](https://platform.openai.com/docs/api-reference/runs/modifyRun) function

```elixir
openai
|> Runs.update(%{
  thread_id: thread_id,
  run_id: run_id,
  metadata: %{user_id: "user_zmVY6FvuBDDwIqM4KgH"}
})
```

### List runs

List the runs belonging to a thread using [`Run.list()`](https://platform.openai.com/docs/api-reference/runs/listRuns)

```elixir
openai |> Runs.list(thread_id)
```

### Submit tool outputs to a run

When a run has the `status`: "requires_action" and `required_action.type` is `submit_tool_outputs`, the [`Run.submit_tool_outputs()`](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs) can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.

```elixir
openai
|> Runs.submit_tool_outputs(%{
  thread_id: thread_id,
  run_id: run_id,
  tool_outputs: [%{tool_call_id: "foobar", output: "28C"}]
})
```

### Cancel a run

You can cancel a run `in_progress` using [`Run.cancel()`](https://platform.openai.com/docs/api-reference/runs/cancelRun)

```elixir
openai |> Runs.cancel(%{thread_id: thread_id, run_id: run_id})
```

### Create thread and run

Use [`Run.create_and_run()`](https://platform.openai.com/docs/api-reference/runs/createThreadAndRun) to create a thread and run.

```elixir

```

<!-- livebook:{"offset":29911,"stamp":{"token":"XCP.rVVvvGrXo18n3oG-jm9hKmnLEVgoE_vzeLpcG4kaqNglXlcKUzVuweX-n8hYzgdwj4JaCjt-dktwRrjXnVXXEJPimmv_VLoHxFdKRdhc9_DRI4_DSrGDuWQ","version":2}} -->
